<!DOCTYPE html>
<html>
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta content="IE=5.0000" http-equiv="X-UA-Compatible">
    <meta name="description" content="Yuwei Niu's home page">
    <link rel="icon" media="(prefers-color-scheme:dark)" href="./assets/imgs/Luffy.jpg" type="image/jpg" />
    <link rel="icon" media="(prefers-color-scheme:light)" href="./assets/imgs/Luffy.jpg" type="image/jpg" />
<!--     <link rel="icon" media="(prefers-color-scheme:light)" href="./assets/imgs/favicon.png" type="image/png" /> -->
    <link href="./profile.css" rel="stylesheet" type="text/css">
    <title>Yuwei Niu's Homepage</title>
    <meta name="GENERATOR" content="MSHTML 11.00.10570.1001">
</head>

<body>
    <div id="layout-content" style="margin-top: 25px;">
        <table>
            <tbody>
                <tr>
                    <td width="670">
                        <div id="toptitle">
                            <h1>Yuwei Niu</h1>
                        </div>
                        <h3>ðŸŽ“ Undergraduate Student</h3>
                        <p>
                            <a href="https://www.cqu.edu.cn/">Chongqing University</a>,
                            <br>Chongqing, China
                            <br>
                            <br> Email:
                            <a href="mailto:niuyuwei04@gmail.com">niuyuwei04@gmail.com</a>
                            <br> WeChat: purshow
                            <br>
                            <br>
                            [<a href="https://scholar.google.com/citations?user=VaYRTxEAAAAJ&hl=zh-CN&oi=ao">Google Scholar</a>]&nbsp;&nbsp;[<a href="https://github.com/purshow">GitHub</a>]
                        </p>
                    </td>
                    <td>
                        <div>
                            <img width="270" src="./assets/imgs/Luffy.jpg" border="0">
                        </div>
                    </td>
                </tr>
                <tr></tr>
            </tbody>
        </table>

        <div id="layout-content" style="margin-top: 25px;">
            <h2>About Me</h2>
            <p>
I am a <b>third-year</b> undergraduate student in the <b>Hongshen Stand-Out Class of Computer Science</b> at <b>Chongqing University</b>. I am incredibly fortunate to have closely collaborated with <b>Prof. <a href="https://scholar.google.com/citations?user=KomQOFkAAAAJ&hl=zh-CN&oi=ao">Lei Feng</a></b> during my undergraduate studies and will be joining <b>Peking University</b>, advised by <b>Prof. <a href="https://scholar.google.com/citations?user=-5juAR0AAAAJ&hl=zh-CN">Li Yuan</a></b>.            </p>
            <p>
                My core research focuses on exploring how to effectively align and fuse "real-world" visual representations with "human-world" language priors. In my view, language itself is a high-dimensional compression and abstract extraction of visual information. It encodes massive amounts of information into more efficient and meaningful representations through learning and refinement. My research direction is precisely to find an effective path for this compression and alignment between the language and visual modalities.
            </p>
            <p>
                Language, as the carrier of human knowledge and experience, forms a microcosm of the "human world"; while vision directly captures and reflects the true appearance of the "real world." I firmly believe that bridging the gap between the language modality, which carries "human world" prior knowledge, and the visual modality, which directly reflects the "real world," is a crucial step towards achieving general artificial intelligence. This not only involves finding effective mapping paths between modalities but also concerns how to leverage their respective strengths to collaboratively build a unified cognitive framework.
            </p>
            <p>
                Currently, I am dedicated to building unified multimodal models and exploring why such models are essential.
            </p>

            <h2>Education</h2>
            <ul>
                <li>
                    [2022-2026] B.Eng. in Computer Science, <b>Hongshen Stand-Out Class</b>, Chongqing University (CQU).
                </li>
            </ul>
<!-- 
            <h2>Research Experience</h2>
            <ul>
                <li>
                    [Dec 2024 - Now] Research Assistant, Peking University.
                    <br>Researching on building unified multimodal large models, supervised by Researcher <a href="https://scholar.google.com/citations?user=-5juAR0AAAAJ">Li Yuan</a>.
                </li>
                <li>
                    [Oct 2023 - Dec 2024] Research Assistant, Nanyang Technological University.
                    <br>Researching on building trustworthy multimodal models, supervised by Prof. <a href="https://scholar.google.com/citations?user=KomQOFkAAAAJ">Lei Feng</a>.
                </li>
            </ul> -->

<h2>Selected Publications & Manuscripts</h2>
            * Equal Contribution &nbsp;&nbsp; # Project Lead

            <ul>
                <li style="margin-top: 0px;">
                    <div style="margin-top: 5px;"><b>ðŸ”¥ WISE: A World Knowledge-Informed Semantic Evaluation for Text-to-Image Generation</b></div>
                    <div style="margin-top: 0px;"><a style="color: #777;"><u><b>Yuwei Niu*</b></u>, Munan Ning*, Mengren Zheng, Bin Lin, Peng Jin, Jiaqi Liao, Kunpeng Ning, Bin Zhu, Li Yuanâ€ .</a></div>
                    <div style="margin-top: 0px;"><a style="color: #b70505c0;"><i>Probing Language Priors in Multimodal Unify Models. First to explore how world knowledge can benefit the understanding and generation of unified models. Widely adopted by many prominent researchers.</i></a></div>
                    <div style="margin-top: 2px;">
                        <i><b>Under Review</b></i> &nbsp;
                        [<a href="https://arxiv.org/abs/2503.07265">Paper</a>]
                        [<a href="https://github.com/PKU-YuanGroup/WISE">Code ðŸŒŸ100+</a>]
                    </div>
                </li>
            </ul>

            <ul>
                <li style="margin-top: 0px;">
                    <div style="margin-top: 5px;"><b>ðŸ”¥ LangBridge: Interpreting Image as a Combination of Language Embeddings</b></div>
                    <div style="margin-top: 0px;"><a style="color: #777;">Jiaqi Liao*, <u><b>Yuwei Niu*</b></u>, Fanqing Meng*, Hao Li, Changyao Tian, Yinuo Du, Yuwen Xiong, Dianqi Li, Xizhou Zhu, Li Yuan, Jifeng Daiâ€ , Yu Chengâ€ .</a></div>
                    <div style="margin-top: 0px;"><a style="color: #b70505c0;"><i>Language Priors as Visual Representations. Introducing a novel multimodal alignment paradigm that competes with LLaVA.</i></a></div>
                    <div style="margin-top: 2px;">
                        <i><b>Under Review</b></i> &nbsp;
                        [<a href="https://arxiv.org/abs/2503.19404">Paper</a>]
                        [<a href="https://jiaqiliao77.github.io/LangBridge.github.io/">Code</a>]
                    </div>
                </li>
            </ul>

            <ul>
                <li style="margin-top: 0px;">
                    <div style="margin-top: 5px;"><b>ðŸ”¥ UniWorld: High-Resolution Semantic Encoders for Unified Visual Understanding and Generation</b></div>
                    <div style="margin-top: 0px;"><a style="color: #777;">Bin Lin, Zongjian Li, Xinhua Cheng, <u><b>Yuwei Niu</b></u>, Yang Ye, Xianyi He, Shenghai Yuan, Wangbo Yu, Shaodong Wang, Yunyang Ge, Yatian Pang, Li Yuanâ€ .</a></div>
                    <div style="margin-top: 0px;"><a style="color: #b70505c0;"><i>A comprehensive and powerful unified model!</i></a></div>
                    <div style="margin-top: 2px;">
                        <i><b>Tech Report</b></i> &nbsp;
                        [<a href="https://arxiv.org/abs/2506.03147">Paper</a>]
                        [<a href="https://github.com/PKU-YuanGroup/UniWorld-V1">Code ðŸŒŸ500+</a>]
                    </div>
                </li>
            </ul>

            <h2>Other Works</h2>
            <ul>
                <li style="margin-top: 0px;">
                    <div style="margin-top: 5px;"><b>Test-Time Multimodal Backdoor Detection by Contrastive Prompting</b></div>
                    <div style="margin-top: 0px;"><a style="color: #777;"><u><b>Yuwei Niu*</b></u>, Shuo He*, Qi Wei, Zongyu Wu, Feng Liu, Lei Fengâ€ .</a></div>
                    <div style="margin-top: 2px;">
                        <i><b>ICML 2025</b></i> &nbsp;
                        [<a href="https://arxiv.org/abs/2405.15269">Paper</a>]
                    </div>
                </li>
            </ul>

            <ul>
                <li style="margin-top: 0px;">
                    <div style="margin-top: 5px;"><b>ICT: Image-Object Cross-Level Trusted Intervention for Mitigating Object Hallucination in Large Vision-Language Models</b></div>
                    <div style="margin-top: 0px;"><a style="color: #777;">Junzhe Chen, Tianshu Zhang, Shiyu Huang, <u><b>Yuwei Niu</b></u>, Linfeng Zhang, Lijie Wen, Xuming Huâ€ .</a></div>
                    <div style="margin-top: 2px;">
                        <i><b>CVPR 2025</b></i> &nbsp;
                        [<a href="https://arxiv.org/abs/2411.15268">Paper</a>]
                    </div>
                </li>
            </ul>

            <ul>
                <li style="margin-top: 0px;">
                    <div style="margin-top: 5px;"><b>Tuning Vision-Language Models with Candidate Labels by Prompt Alignment</b></div>
                    <div style="margin-top: 0px;"><a style="color: #777;">Zhifang Zhang, <u><b>Yuwei Niu</b></u>, Xin Liu, Beibei Liâ€ .</a></div>
                    <div style="margin-top: 2px;">
                        <i><b>DASFAA 2025</b></i> &nbsp;
                        [<a href="https://arxiv.org/abs/2407.07638">Paper</a>]
                    </div>
                </li>
            </ul>
            <ul>
                <li style="margin-top: 0px;">
                    <div style="margin-top: 5px;"><b>LanP: Rethinking the Impact of Language Priors in Large Vision-Language Models</b></div>
                    <div style="margin-top: 0px;"><a style="color: #777;">Zongyu Wu*, <u><b>Yuwei Niu*</b></u>, Hongcheng Gao, Minhua Lin, Zhiwei Zhang, Zhifang Zhang, Qi Shi, Yilong Wang, Sike Fu, Junjie Xu, Junjie Ao, Enyan Dai, Lei Feng, Xiang Zhang, Suhang Wangâ€ .</a></div>
                    <div style="margin-top: 0px;"><a style="color: #b70505c0;"><i>Probing Language Priors in Multimodal Models.</i></a></div>
                    <div style="margin-top: 2px;">
                        [<a href="https://arxiv.org/abs/2502.12359">Paper</a>]
                    </div>
                </li>
            </ul>
            <ul>
                <li style="margin-top: 0px;">
                    <div style="margin-top: 5px;"><b>OMNIDPO: A Preference Optimization Framework to Address Omni-Modal Hallucination</b></div>
                    <div style="margin-top: 0px;"><a style="color: #777;">Junzhe Chen, Tianshu Zhang, Shiyu Huang, <u><b>Yuwei Niu</b></u>, Rongzhou Zhang, Guanyu Zhou, Lijie Wen, Xuming Huâ€ .</a></div>
                    <div style="margin-top: 2px;">
                        <i><b>Under Review</b></i> &nbsp;
                        </div>
                </li>
            </ul>
            <h2>Community Contribution</h2>
            <ul>
                <li>
                    <b>Founder of <a href="https://github.com/Purshow/Awesome-Unified-Multimodal">Awesome-Unified-Multimodal</a></b>
                    <br>Created the most comprehensive repository for organizing papers, codes, and resources on unified multimodal models. (<a href="https://github.com/Purshow/Awesome-Unified-Multimodal">GitHub ðŸŒŸ220+</a>)
                </li>
            </ul>

            <h2>Personality</h2>
            <p>
                Beyond science, I am deeply passionate about literature, poetry, anime, films, music, and all forms of art that embody human creativity. I firmly believe these are the reasons for our existence. Currently, I am captivated by Dostoevsky's literature and R&B music.
            </p>


            <h2>Contact Me</h2>
            <p>
                I truly believe that great ideas and improvements come from open discussions and debates in academia. If you have any thoughts, disagreements with my work, or fresh ideas youâ€™d like to share, Iâ€™d be really grateful to hear from you. I am incredibly fortunate to have met many friends who have helped me along the way, and I am always willing to chat and offer any assistance I can.
            </p>
            <p>
                If youâ€™ve got any questions about my research or if youâ€™ve tried reaching out through GitHub issues and havenâ€™t heard back, please donâ€™t hesitate to drop me an email. Iâ€™m always here to chat or help out in any way I can.
            </p>
            <p>
                Please note that I am only interested in discussing intriguing problems and insights, not metrics. If you are inclined to discuss publication or citation numbers, rely on numerical indicators to quantify individuals, or compare me to others, please refrain from contacting me.
            </p>
            <p>My preferred email: <a href="mailto:niuyuwei04@gmail.com">niuyuwei04@gmail.com</a></p>


        </div>
    </div>

</body>
</html>
